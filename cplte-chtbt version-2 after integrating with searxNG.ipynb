{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":193958,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":165413,"modelId":187734}],"dockerImageVersionId":30823,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"! pip install -q -U transformers\n! pip install -q -U peft\n! pip install -q -U bitsandbytes\n! pip install -q -U datasets\n! pip install -q -U trl\n! pip install -q -U accelerate\n! pip install -q -U wandb\n! pip install -q -U psutil\n! pip install  -q  GPUtil\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-19T07:09:41.793473Z","iopub.execute_input":"2024-12-19T07:09:41.793752Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.1/44.1 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.1/10.1 MB\u001b[0m \u001b[31m69.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m76.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.8/374.8 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m450.5/450.5 kB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.1/69.1 MB\u001b[0m \u001b[31m25.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m293.4/293.4 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m336.4/336.4 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m287.3/287.3 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ndistributed 2024.8.0 requires dask==2024.8.0, but you have dask 2024.12.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"import os\nimport torch\nfrom datasets import load_dataset,Dataset, DatasetDict, load_from_disk\nfrom transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    BitsAndBytesConfig,\n    HfArgumentParser,\n    TrainingArguments,\n    pipeline,\n    logging,\n)\nimport transformers\nfrom transformers.integrations import WandbCallback\nimport random\nfrom peft import LoraConfig, PeftModel\nfrom trl import SFTTrainer\nprint('import bnb')\nimport bitsandbytes as bnb\n# from GPUtil import showUtilization as gpu_usage\nimport wandb\nfrom tqdm.notebook import tqdm\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import ListedColormap, LinearSegmentedColormap\nimport seaborn as sns\nimport pandas as pd\nimport time\nimport math\nimport gc","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T07:15:27.924806Z","iopub.execute_input":"2024-12-19T07:15:27.925103Z","iopub.status.idle":"2024-12-19T07:15:44.141358Z","shell.execute_reply.started":"2024-12-19T07:15:27.925080Z","shell.execute_reply":"2024-12-19T07:15:44.140667Z"}},"outputs":[{"name":"stdout","text":"import bnb\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"print('torch version: ', torch.__version__)\nprint(f'transformers version: {transformers.__version__}')\nprint(f'bnb version: {bnb.__version__}')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class CFG:\n  # The model that you want to train from the Hugging Face hub\n  # model_name = \"NousResearch/Llama-2-7b-chat-hf\"\n  #model_name = 'meta-llama/Llama-2-7b-hf'\n  model_name = 'NousResearch/Llama-2-7b-chat-hf'\n  seed = 42\n  val_split = 0.1\n  debug = True\n  resume_train = False\n\n  # The instruction dataset to use\n  dataset_name = \"Sathya-20/Filtered_Medical_Dataset_final\"\n\n\n  project= 'LLM finetuning'\n\n\n  ################################################################################\n  # QLoRA parameters\n  ################################################################################\n\n  # LoRA attention dimension\n  lora_r = 8\n\n  # Alpha parameter for LoRA scaling\n  lora_alpha = 16\n\n  # Dropout probability for LoRA layers\n  lora_dropout = 0.1\n\n  ################################################################################\n  # bitsandbytes parameters\n  ################################################################################\n\n  # Activate 4-bit precision base model loading\n  use_4bit = True\n\n  # Compute dtype for 4-bit base models\n  bnb_4bit_compute_dtype = \"float16\"\n\n  # Quantization type (fp4 or nf4)\n  bnb_4bit_quant_type = \"nf4\"\n\n  # Activate nested quantization for 4-bit base models (double quantization)\n  use_nested_quant = False\n\n  ################################################################################\n  # TrainingArguments parameters\n  ################################################################################\n\n  # Output directory where the model predictions and checkpoints will be stored\n  output_dir = './Checkpoints/'#\"./results\"\n  report_path = './report_table.csv'\n\n  # Number of training epochs\n  num_train_epochs = 1\n\n  # Enable fp16/bf16 training (set bf16 to True with an A100)\n  fp16 = True\n  bf16 = False\n\n  # Batch size per GPU for training\n  per_device_train_batch_size = 4\n\n  # Batch size per GPU for evaluation\n  per_device_eval_batch_size = 4\n\n  # Number of update steps to accumulate the gradients for\n  gradient_accumulation_steps = 2\n\n  # Enable gradient checkpointing\n  gradient_checkpointing = True\n\n  # Maximum gradient normal (gradient clipping)\n  max_grad_norm = 0.3\n\n  # Initial learning rate (AdamW optimizer)\n  learning_rate = 2e-4\n\n  # Weight decay to apply to all layers except bias/LayerNorm weights\n  weight_decay = 0.001\n\n  # Optimizer to use\n  optim = \"paged_adamw_32bit\"\n\n  # Learning rate schedule\n  lr_scheduler_type = \"constant\"\n\n\n\n  # Ratio of steps for a linear warmup (from 0 to learning rate)\n  warmup_ratio = 0.03\n\n  # Group sequences into batches with same length\n  # Saves memory and speeds up training considerably\n  group_by_length = True\n\n  # Number of training steps (overrides num_train_epochs)\n  max_steps = 10 if debug else 100\n\n  run_name = model_name.split('/')[-1] + '-' + dataset_name.split('/')[-1] + f'_{max_steps}_steps'\n  max_steps = max_steps + [int(x.split('-')[-1]) for x in os.listdir(output_dir) if 'checkpoint' in x][0] if resume_train else max_steps\n\n\n\n  # Number of updates steps before saving checkpoint\n  save_steps = 1 if debug else 5\n\n  # Save checkpoint every X updates steps\n  save_steps = 1 if debug else 5\n\n  # Log every X updates steps\n  logging_steps = 1 if debug else 5\n\n  log_table_steps = [x for x in range(1, max_steps, math.floor(max_steps/3))]\n\n  ################################################################################\n  # SFT parameters\n  ################################################################################\n\n  # Maximum sequence length to use\n  max_seq_length = 1024\n\n  # Pack multiple short examples in the same input sequence to increase efficiency\n  packing = True\n\n  # Load the entire model on the GPU 0\n  device_map = 'auto'","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class clr:\n    S = '\\033[1m' + '\\033[94m'\n    E = '\\033[0m'\n    R = '\\033[31m'\n    G = '\\033[1;32m'\n    Y = '\\033[33m'\n\nmy_colors = [\"#5EAFD9\", \"#449DD1\", \"#3977BB\",\n             \"#2D51A5\", \"#5C4C8F\", \"#8B4679\",\n             \"#C53D4C\", \"#E23836\", \"#FF4633\", \"#FF5746\"]\nCMAP1 = ListedColormap(my_colors)\n\nprint(clr.S+\"Notebook Color Schemes:\"+clr.E)\nsns.palplot(sns.color_palette(my_colors))\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport wandb\n\n# Set environment variable\nos.environ[\"WANDB_API_KEY\"] = \"d62413290a366601bd34f6b1f9e6d0c983820a3d\"\n\n# Now login without needing to provide the key directly\nwandb.login()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"wandb.init(\n    project = CFG.project,\n    name = CFG.run_name,\n    resume= CFG.resume_train,\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load dataset (you can process it here)\ndataset = load_dataset(CFG.dataset_name, split = 'train')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def format_text(text,line_lenght= 15):\n  print_word = ''\n  for i,word in enumerate(text.split(' ')):\n    print_word+=word + ' '\n    i += 1\n    print_word =print_word + '\\n' if i%line_lenght == 0 else print_word\n\n  return print_word","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(clr.S+'Prompt'+clr.E)\nprint(format_text(dataset['Query'][0]))\nprint(clr.S+'\\nCompletition'+clr.E)\nprint(format_text(dataset['Response'][0]))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"formated_text = {'text':[]}\n\nbase_string = '''[INST]\nBelow is an instruction that describes a task.\nWrite a response that appropriately completes the request.\\n\\n\n{user_prompt}\\n\\n\n[/INST] {completition} </s>'''\n\nsystem_prompt = '''Below is an instruction that describes a task.\nWrite a response that appropriately completes the request.\\n\\n '''\n\ndataset = dataset.shuffle(seed=random.seed(CFG.seed))\n\nfor i,(Query, Response) in enumerate(zip(dataset['Query'],dataset['Response'])):\n    text = base_string.format(user_prompt = Query,\n                              completition = Response\n                              )\n    if i == 0:\n      text_example = clr.S+'[INST]\\n'+clr.E +\\\n            system_prompt + Query + '\\n\\n' +\\\n            clr.S+'[/INST] '+clr.E + Response + clr.S+' </s>'+clr.E\n\n    formated_text['text'].append(text)\n\n\nformate_dataset = Dataset.from_dict(formated_text)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"n_data = 10000 if CFG.debug else len(formate_dataset)\nn_data_train = int((1 - CFG.val_split) * n_data)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_dataset = Dataset.from_dict(formate_dataset[:n_data_train])\nvalid_dataset = Dataset.from_dict(formate_dataset[n_data_train:n_data])\n\nprint(clr.S+'Debug Mode = '+clr.E, CFG.debug)\nprint(clr.S+'\\nData Size'+clr.E)\nprint('Train = ',len(train_dataset), '| Valid = ', len(valid_dataset))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"### save HF Dataset to disk\ntrain_dataset.save_to_disk('./MedText-train')\nvalid_dataset.save_to_disk('./MedText-valid')\n### load HF Dataset from disk\ntrain_dataset = load_from_disk('./MedText-train')\nvalid_dataset = load_from_disk('./MedText-valid')\ntrain_dataset, valid_dataset","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"compute_dtype = getattr(torch, CFG.bnb_4bit_compute_dtype)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if compute_dtype == torch.float16 and CFG.use_4bit:\n    major, _ = torch.cuda.get_device_capability()\n    if major >= 8:\n        print(\"=\" * 80)\n        print(\"Your GPU supports bfloat16: accelerate training with bf16=True\")\n        print(\"=\" * 80)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"num_gpus = torch.cuda.device_count()\n# Get information about each GPU\nprint('General Report')\nfor i in range(num_gpus):\n    gpu_properties = torch.cuda.get_device_properties(i)\n    print(f\"\\nGPU {i}: {gpu_properties.name}\")\n    print(f\"  Total Memory: {gpu_properties.total_memory / (1024**3):.2f} GB\")\n    print(f\"  CUDA Version: {gpu_properties.major}.{gpu_properties.minor}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"bnb_config = BitsAndBytesConfig(\n    load_in_4bit=CFG.use_4bit,\n    bnb_4bit_quant_type=CFG.bnb_4bit_quant_type,\n    bnb_4bit_compute_dtype=compute_dtype,\n    bnb_4bit_use_double_quant=CFG.use_nested_quant,\n)\nprint('BitsAndBytesConfig\\n')\nbnb_config.to_dict()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#https://www.kaggle.com/code/hinepo/llm-instruction-finetuning-wandb#Model\ndef print_trainable_parameters(model):\n    \"\"\"\n    Prints the number of trainable parameters in the model.\n    \"\"\"\n    trainable_params = 0\n    all_param = 0\n    for _, param in model.named_parameters():\n        all_param += param.numel()\n        if param.requires_grad:\n            trainable_params += param.numel()\n\n    formatted_trainable_params = \"{:,}\".format(trainable_params).replace(\",\", \"_\")\n    formatted_all_param = \"{:,}\".format(all_param).replace(\",\", \"_\")\n    print(\n        f\"trainable params: {formatted_trainable_params} || all params: {formatted_all_param} || trainable: {round(100 * trainable_params / all_param, 2)} %\"\n    )","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%time\nmodel = AutoModelForCausalLM.from_pretrained(\n    CFG.model_name,\n    quantization_config=bnb_config,\n    device_map=CFG.device_map\n)\n\nmodel.config.use_cache = False\nmodel.config.pretraining_tp = 1\nprint_trainable_parameters(model)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(\n                            CFG.model_name,\n                            trust_remote_code=True\n                            )\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"right\" # Fix weird overflow issue with fp16 training","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.hf_device_map","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"layer_names = list(model.state_dict().keys())\n\nfor name in layer_names[4:10]:\n    print(name)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def find_all_linear_names(model):\n    cls = bnb.nn.Linear4bit\n    lora_module_names = set()\n    for name, module in model.named_modules():\n        if isinstance(module, cls):\n            names = name.split('.')\n            lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n    if 'lm_head' in lora_module_names:\n        lora_module_names.remove('lm_head')\n    return list(lora_module_names)\nlinear_layers = find_all_linear_names(model)\nprint(f\"Linear layers in the model: {linear_layers}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"peft_config = LoraConfig(\n    lora_alpha=CFG.lora_alpha,\n    lora_dropout=CFG.lora_dropout,\n    target_modules = linear_layers,\n    r=CFG.lora_r,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n)\npeft_config.to_dict()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.add_adapter(peft_config)\nprint_trainable_parameters(model)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"training_arguments = TrainingArguments(\n    output_dir=CFG.output_dir,\n    num_train_epochs=CFG.num_train_epochs,\n    per_device_train_batch_size=CFG.per_device_train_batch_size,\n    per_device_eval_batch_size=CFG.per_device_eval_batch_size,\n    gradient_accumulation_steps=CFG.gradient_accumulation_steps,\n    optim=CFG.optim,\n    learning_rate=CFG.learning_rate,\n    weight_decay=CFG.weight_decay,\n    fp16=CFG.fp16,\n    bf16=CFG.bf16,\n    max_grad_norm=CFG.max_grad_norm,\n    max_steps=CFG.max_steps,\n    warmup_ratio=CFG.warmup_ratio,\n    lr_scheduler_type=CFG.lr_scheduler_type,\n    save_strategy = 'steps',\n    save_steps=CFG.save_steps,\n    save_total_limit = 1,\n    gradient_checkpointing = True,\n    logging_strategy = \"steps\",\n    logging_steps=CFG.logging_steps,\n    evaluation_strategy = \"steps\",\n    eval_steps = CFG.logging_steps,\n    save_safetensors = True,\n    report_to=\"wandb\",\n    seed = CFG.seed,\n    data_seed = CFG.seed,\n    push_to_hub = False,\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class LLMSampleCB(WandbCallback):\n    def __init__(self, trainer, test_dataset, num_samples=10, max_new_tokens=256):\n        \"A Callback to log samples to a wandb.Table during training\"\n        super().__init__()\n\n        # Ensure the `log_model` is not passed as a string\n        self._log_model = None  # Use None or a proper model object if needed\n        \n        self.sample_dataset = test_dataset.select(range(num_samples))\n        self.model, self.tokenizer = trainer.model, trainer.tokenizer\n        \n        # Define the generation config\n        self.gen_config = transformers.GenerationConfig.from_pretrained(\n            trainer.model.name_or_path, max_new_tokens=max_new_tokens\n        )\n\n        # Initialize step tracking (to handle logging steps)\n        self.step = 1 if not os.path.exists(CFG.report_path) else pd.read_csv(CFG.report_path)['step'].max() + 1\n\n    def generate(self, instruction):\n        \"\"\"Generate a response for the given instruction.\"\"\"\n        tokenized_prompt = self.tokenizer(instruction, return_tensors='pt', padding=True).to('cuda')\n\n        with torch.inference_mode():\n            output = self.model.generate(\n                **tokenized_prompt,\n                generation_config=self.gen_config,\n                pad_token_id=self.tokenizer.eos_token_id\n            )\n\n        return self.tokenizer.decode(output[0][len(tokenized_prompt[0]):], skip_special_tokens=True)\n\n    def samples_table(self, examples):\n        \"\"\"Create a wandb.Table to store the generations.\"\"\"\n        if self.step in CFG.log_table_steps:\n            data = []\n            for example in examples:\n                prompt = example[\"text\"]\n                instruction = prompt.split('[/INST]')[0][6:]\n                response = prompt.split('[/INST]')[1][:-5]\n                generation = self.generate(instruction=instruction)\n                data.append([instruction, response, generation, *list(self.gen_config.to_dict().values())])\n\n            rerport_step_df = pd.DataFrame(data, columns=['instruction', 'response', 'generation'] + list(self.gen_config.to_dict().keys()))\n        else:\n            rerport_step_df = None\n\n        self.step += 1\n        return rerport_step_df\n\n    def on_evaluate(self, args, state, control, **kwargs):\n        \"\"\"Log the wandb.Table after calling trainer.evaluate.\"\"\"\n        super().on_evaluate(args, state, control, **kwargs)\n        \n        rerport_step_df = self.samples_table(self.sample_dataset)\n\n        # Update the table with predictions in the new step  \n        if rerport_step_df is not None:\n            if self.step - 1 == 1:\n                rerport_step_df['step'] = self.step - 1\n                rerport_step_df.to_csv(CFG.report_path, index=False)\n                new_report_df = rerport_step_df.copy()\n            else:\n                previous_report_df = pd.read_csv(CFG.report_path)\n                rerport_step_df['step'] = self.step - 1\n                new_report_df = pd.concat([previous_report_df, rerport_step_df], ignore_index=True)\n                new_report_df.to_csv(CFG.report_path, index=False)\n\n            table = wandb.Table(dataframe=new_report_df, allow_mixed_types=True)\n            wandb.log({\"sample_predictions\": table})","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Set supervised fine-tuning parameters\ntrainer = SFTTrainer(\n    model=model,\n    train_dataset=train_dataset,\n    eval_dataset = valid_dataset,\n    peft_config=peft_config,\n    dataset_text_field=\"text\",\n    max_seq_length=CFG.max_seq_length,\n    tokenizer=tokenizer,\n    args=training_arguments,\n    packing=CFG.packing,\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"wandb_callback = LLMSampleCB(trainer,\n                             valid_dataset,\n                             num_samples=5,\n                             max_new_tokens= 256)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"trainer.add_callback(wandb_callback)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_dataset","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(clr.S + 'All Train Steps = ' + clr.E, len(train_dataset)/CFG.per_device_train_batch_size)\nprint(clr.S + 'Max Train Steps = ' + clr.E, CFG.max_steps)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if not os.path.exists(CFG.output_dir):\n    os.makedirs(CFG.output_dir)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom peft import PeftModel\n# Path to the model checkpoint\ncheckpoint_path = \"/kaggle/input/krisssssssss/other/default/1/checkpoint-10\"\n# Load the base model and tokenizer\nbase_model_name = \"NousResearch/Llama-2-7b-chat-hf\"  # Replace with the base model used for finetuning\ntokenizer = AutoTokenizer.from_pretrained(checkpoint_path, trust_remote_code=True)\nmodel = AutoModelForCausalLM.from_pretrained(base_model_name, device_map=\"auto\")\n# Load the LoRA adapter\nmodel = PeftModel.from_pretrained(model, checkpoint_path)\nmodel.eval()\n\n# Define the system prompt template\nbase_string = '''[INST]\nBelow is an instruction that describes a task.\nWrite a response that appropriately completes the request.\\n\\n\n{user_prompt}\\n\\n\n[/INST]'''\n\n# Perform inference\ndef generate_response(prompt, max_new_tokens=256):\n    # Format the prompt\n    formatted_prompt = base_string.format(user_prompt=prompt)\n\n    # Tokenize the input\n    inputs = tokenizer(formatted_prompt, return_tensors=\"pt\", padding=True, truncation=True).to(\"cuda\")\n\n    # Generate response\n    with torch.no_grad():\n        outputs = model.generate(\n            input_ids=inputs[\"input_ids\"],\n            attention_mask=inputs[\"attention_mask\"],\n            max_new_tokens=max_new_tokens,\n            temperature=0.7,\n            top_k=50,\n            top_p=0.9,\n            pad_token_id=tokenizer.eos_token_id\n        )\n    # Decode and return the response\n    return tokenizer.decode(outputs[0][inputs[\"input_ids\"].shape[1]:], skip_special_tokens=True)\n\n# Example usage\nprompt = \"What are the symptoms of a heart attack?\"\nresponse = generate_response(prompt)\nprint(\"User Prompt:\", prompt)\nprint(\"Model Response:\", response)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T06:28:53.840609Z","iopub.execute_input":"2024-12-19T06:28:53.840911Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b30d652568fa45d9a6b36b55fd19fa3c"}},"metadata":{}}],"execution_count":null},{"cell_type":"code","source":"!pip install -q langchain langchain-community transformers peft datasets\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T07:33:45.770883Z","iopub.execute_input":"2024-12-19T07:33:45.771218Z","iopub.status.idle":"2024-12-19T07:33:49.117501Z","shell.execute_reply.started":"2024-12-19T07:33:45.771190Z","shell.execute_reply":"2024-12-19T07:33:49.116657Z"}},"outputs":[{"name":"stderr","text":"/usr/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"from langchain_community.utilities import SearchApiAPIWrapper\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Install required libraries\n!pip install -q -U transformers\n!pip install -q -U peft\n!pip install -q -U bitsandbytes\n!pip install -q -U datasets\n!pip install -q -U trl\n!pip install -q -U accelerate\n!pip install -q -U langchain\n!pip install -q -U langchain-utilities\n!pip install -q -U wandb\n!pip install -q -U psutil\n!pip install -q GPUtil","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T14:11:23.697049Z","iopub.execute_input":"2024-12-19T14:11:23.697364Z","iopub.status.idle":"2024-12-19T14:12:22.693056Z","shell.execute_reply.started":"2024-12-19T14:11:23.697339Z","shell.execute_reply":"2024-12-19T14:12:22.692042Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.1/44.1 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.1/10.1 MB\u001b[0m \u001b[31m76.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m83.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.8/374.8 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m450.5/450.5 kB\u001b[0m \u001b[31m21.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.1/69.1 MB\u001b[0m \u001b[31m25.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m293.4/293.4 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m336.4/336.4 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.8/41.8 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m411.5/411.5 kB\u001b[0m \u001b[31m29.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m320.7/320.7 kB\u001b[0m \u001b[31m24.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.5/73.5 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m131.3/131.3 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h\u001b[31mERROR: Could not find a version that satisfies the requirement langchain-utilities (from versions: none)\u001b[0m\u001b[31m\n\u001b[0m\u001b[31mERROR: No matching distribution found for langchain-utilities\u001b[0m\u001b[31m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m287.3/287.3 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ndistributed 2024.8.0 requires dask==2024.8.0, but you have dask 2024.12.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Building wheel for GPUtil (setup.py) ... \u001b[?25l\u001b[?25hdone\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!pip install -q langchain langchain-community transformers peft datasets\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T14:15:58.320381Z","iopub.execute_input":"2024-12-19T14:15:58.320709Z","iopub.status.idle":"2024-12-19T14:16:04.055238Z","shell.execute_reply.started":"2024-12-19T14:15:58.320683Z","shell.execute_reply":"2024-12-19T14:16:04.054133Z"}},"outputs":[{"name":"stderr","text":"/usr/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\n","output_type":"stream"},{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m32.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# Import necessary modules\nimport os\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom peft import PeftModel\nfrom langchain_community.utilities import SearchApiAPIWrapper\n# Set up API key for SearxNG search\nos.environ[\"SEARCHAPI_API_KEY\"] = \"FuP4MaY7CFaenRnr53jSEaqF\"\n# Initialize SearxNG search API wrapper\nsearch = SearchApiAPIWrapper()\n# Load fine-tuned model and tokenizer\ncheckpoint_path = \"/kaggle/input/krisssssssss/other/default/1/checkpoint-10\"  # Replace with your checkpoint path\nbase_model_name = \"NousResearch/Llama-2-7b-chat-hf\"  # Replace with your base model\ntokenizer = AutoTokenizer.from_pretrained(checkpoint_path, trust_remote_code=True)\nmodel = AutoModelForCausalLM.from_pretrained(base_model_name, device_map=\"auto\")\nmodel = PeftModel.from_pretrained(model, checkpoint_path)\nmodel.eval()\n# Define the system prompt template\nbase_string = '''[INST]\nBelow is an instruction that describes a task.\nWrite a response that appropriately completes the request.\\n\\n\n{user_prompt}\\n\\n\n[/INST]'''\n# Function to generate chatbot responses\ndef generate_response(prompt, max_new_tokens=256):\n    \"\"\"\n    Generate a response from the fine-tuned model.\n    \"\"\"\n    formatted_prompt = base_string.format(user_prompt=prompt)\n    inputs = tokenizer(formatted_prompt, return_tensors=\"pt\", padding=True, truncation=True).to(\"cuda\")\n    with torch.no_grad():\n        outputs = model.generate(\n            input_ids=inputs[\"input_ids\"],\n            attention_mask=inputs[\"attention_mask\"],\n            max_new_tokens=max_new_tokens,\n            temperature=0.7,\n            top_k=50,\n            top_p=0.9,\n            pad_token_id=tokenizer.eos_token_id\n        )\n    return tokenizer.decode(outputs[0][inputs[\"input_ids\"].shape[1]:], skip_special_tokens=True)\n\n# Function to synthesize the final response using prompting techniques\ndef evaluate_and_synthesize(chatbot_response, search_results):\n    \"\"\"\n    Compare chatbot's response and search results to generate the best result.\n    \"\"\"\n    evaluation_prompt = f\"\"\"\n    Chatbot Response: {chatbot_response}\n    Search Results: {search_results}\n\n    Based on the above responses, decide which one provides the most accurate, up-to-date, and comprehensive answer \n    to the user's query. If there are relevant updates from the Search Results, incorporate them into the Chatbot's Response.\n    Return a single, consolidated response that combines accuracy and relevance.\n    \"\"\"\n    final_response = generate_response(evaluation_prompt)\n    return final_response\n\n# Function to handle user queries\ndef consolidate_responses(user_query):\n    \"\"\"\n    Compare chatbot and SearxNG responses, synthesize the final answer.\n    \"\"\"\n    chatbot_response = generate_response(user_query)\n    search_results = search.run(user_query, engines=[\"wiki\"])  # Specify search engines as needed\n    if search_results:\n        final_response = evaluate_and_synthesize(chatbot_response, search_results)\n    else:\n        final_response = chatbot_response  # Use chatbot response if no search results are available\n    return final_response\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T14:16:29.412882Z","iopub.execute_input":"2024-12-19T14:16:29.413241Z","iopub.status.idle":"2024-12-19T14:18:16.904338Z","shell.execute_reply.started":"2024-12-19T14:16:29.413210Z","shell.execute_reply":"2024-12-19T14:18:16.903405Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/583 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cd245025bb544beab71b4e5dabeb0308"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/26.8k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"77091596a8da4292af15700741c0baef"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"05d956cfd8fb4b399a806a8968f1abfb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00002.safetensors:   0%|          | 0.00/9.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"44ecec5b01c24ea89ed1b91860f39f76"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00002.safetensors:   0%|          | 0.00/3.50G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dd28499ab1af4ce2a052970488bf6f10"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a21b5abeb2a6486cb02d07ab189c5457"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/200 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fe0709c1dfd04adf87739a85e2651bfe"}},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"# Example query\nuser_query = \"What are the symptoms of a heart attack?\"\n# Get the final response\nfinal_response = consolidate_responses(user_query)\n# Display the response\nprint(\"User Query:\", user_query)\nprint(\"Final Response:\", final_response)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T14:18:45.530787Z","iopub.execute_input":"2024-12-19T14:18:45.531473Z","iopub.status.idle":"2024-12-19T14:19:39.012547Z","shell.execute_reply.started":"2024-12-19T14:18:45.531437Z","shell.execute_reply":"2024-12-19T14:19:39.011578Z"}},"outputs":[{"name":"stderr","text":"Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n","output_type":"stream"},{"name":"stdout","text":"User Query: What are the symptoms of a heart attack?\nFinal Response: \n\nThank you for reaching out! A heart attack is indeed a serious medical emergency that requires immediate attention. The symptoms you mentioned are consistent with a heart attack, and I would advise you to seek medical attention immediately.\n\nIf you are experiencing any of the following symptoms, please call emergency services or go to the nearest hospital:\n\n1. Chest pain or discomfort\n2. Shortness of breath\n3. Lightheadedness or dizziness\n4. Fatigue\n5. Anxiety or panic\n\nRemember, a heart attack can cause permanent damage to your heart muscle if not treated promptly. Don't hesitate to reach out if you have any concerns or questions.\n\nTake care!\n","output_type":"stream"}],"execution_count":6}]}