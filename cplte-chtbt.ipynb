{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"! pip install -q -U transformers\n! pip install -q -U peft\n! pip install -q -U bitsandbytes\n! pip install -q -U datasets\n! pip install -q -U trl\n! pip install -q -U accelerate\n! pip install -q -U wandb\n! pip install -q -U psutil\n! pip install  -q  GPUtil\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport torch\nfrom datasets import load_dataset,Dataset, DatasetDict, load_from_disk\nfrom transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    BitsAndBytesConfig,\n    HfArgumentParser,\n    TrainingArguments,\n    pipeline,\n    logging,\n)\nimport transformers\nfrom transformers.integrations import WandbCallback\nimport random\nfrom peft import LoraConfig, PeftModel\nfrom trl import SFTTrainer\nprint('import bnb')\nimport bitsandbytes as bnb\n\n# from GPUtil import showUtilization as gpu_usage\n\nimport wandb\nfrom tqdm.notebook import tqdm\n\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import ListedColormap, LinearSegmentedColormap\nimport seaborn as sns\n\nimport pandas as pd\nimport time\nimport math\nimport gc","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print('torch version: ', torch.__version__)\nprint(f'transformers version: {transformers.__version__}')\nprint(f'bnb version: {bnb.__version__}')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class CFG:\n  # The model that you want to train from the Hugging Face hub\n  # model_name = \"NousResearch/Llama-2-7b-chat-hf\"\n  #model_name = 'meta-llama/Llama-2-7b-hf'\n  model_name = 'NousResearch/Llama-2-7b-chat-hf'\n  seed = 42\n  val_split = 0.1\n  debug = True\n  resume_train = False\n\n  # The instruction dataset to use\n  dataset_name = \"Sathya-20/Filtered_Medical_Dataset_final\"\n\n\n  project= 'LLM finetuning'\n\n\n  ################################################################################\n  # QLoRA parameters\n  ################################################################################\n\n  # LoRA attention dimension\n  lora_r = 8\n\n  # Alpha parameter for LoRA scaling\n  lora_alpha = 16\n\n  # Dropout probability for LoRA layers\n  lora_dropout = 0.1\n\n  ################################################################################\n  # bitsandbytes parameters\n  ################################################################################\n\n  # Activate 4-bit precision base model loading\n  use_4bit = True\n\n  # Compute dtype for 4-bit base models\n  bnb_4bit_compute_dtype = \"float16\"\n\n  # Quantization type (fp4 or nf4)\n  bnb_4bit_quant_type = \"nf4\"\n\n  # Activate nested quantization for 4-bit base models (double quantization)\n  use_nested_quant = False\n\n  ################################################################################\n  # TrainingArguments parameters\n  ################################################################################\n\n  # Output directory where the model predictions and checkpoints will be stored\n  output_dir = './Checkpoints/'#\"./results\"\n  report_path = './report_table.csv'\n\n  # Number of training epochs\n  num_train_epochs = 1\n\n  # Enable fp16/bf16 training (set bf16 to True with an A100)\n  fp16 = True\n  bf16 = False\n\n  # Batch size per GPU for training\n  per_device_train_batch_size = 4\n\n  # Batch size per GPU for evaluation\n  per_device_eval_batch_size = 4\n\n  # Number of update steps to accumulate the gradients for\n  gradient_accumulation_steps = 2\n\n  # Enable gradient checkpointing\n  gradient_checkpointing = True\n\n  # Maximum gradient normal (gradient clipping)\n  max_grad_norm = 0.3\n\n  # Initial learning rate (AdamW optimizer)\n  learning_rate = 2e-4\n\n  # Weight decay to apply to all layers except bias/LayerNorm weights\n  weight_decay = 0.001\n\n  # Optimizer to use\n  optim = \"paged_adamw_32bit\"\n\n  # Learning rate schedule\n  lr_scheduler_type = \"constant\"\n\n\n\n  # Ratio of steps for a linear warmup (from 0 to learning rate)\n  warmup_ratio = 0.03\n\n  # Group sequences into batches with same length\n  # Saves memory and speeds up training considerably\n  group_by_length = True\n\n  # Number of training steps (overrides num_train_epochs)\n  max_steps = 10 if debug else 100\n\n  run_name = model_name.split('/')[-1] + '-' + dataset_name.split('/')[-1] + f'_{max_steps}_steps'\n  max_steps = max_steps + [int(x.split('-')[-1]) for x in os.listdir(output_dir) if 'checkpoint' in x][0] if resume_train else max_steps\n\n\n\n  # Number of updates steps before saving checkpoint\n  save_steps = 1 if debug else 5\n\n  # Save checkpoint every X updates steps\n  save_steps = 1 if debug else 5\n\n  # Log every X updates steps\n  logging_steps = 1 if debug else 5\n\n  log_table_steps = [x for x in range(1, max_steps, math.floor(max_steps/3))]\n\n  ################################################################################\n  # SFT parameters\n  ################################################################################\n\n  # Maximum sequence length to use\n  max_seq_length = 1024\n\n  # Pack multiple short examples in the same input sequence to increase efficiency\n  packing = True\n\n  # Load the entire model on the GPU 0\n  device_map = 'auto'","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class clr:\n    S = '\\033[1m' + '\\033[94m'\n    E = '\\033[0m'\n    R = '\\033[31m'\n    G = '\\033[1;32m'\n    Y = '\\033[33m'\n\nmy_colors = [\"#5EAFD9\", \"#449DD1\", \"#3977BB\",\n             \"#2D51A5\", \"#5C4C8F\", \"#8B4679\",\n             \"#C53D4C\", \"#E23836\", \"#FF4633\", \"#FF5746\"]\nCMAP1 = ListedColormap(my_colors)\n\nprint(clr.S+\"Notebook Color Schemes:\"+clr.E)\nsns.palplot(sns.color_palette(my_colors))\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport wandb\n\n# Set environment variable\nos.environ[\"WANDB_API_KEY\"] = \"d62413290a366601bd34f6b1f9e6d0c983820a3d\"\n\n# Now login without needing to provide the key directly\nwandb.login()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"wandb.init(\n    project = CFG.project,\n    name = CFG.run_name,\n    resume= CFG.resume_train,\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load dataset (you can process it here)\ndataset = load_dataset(CFG.dataset_name, split = 'train')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def format_text(text,line_lenght= 15):\n  print_word = ''\n  for i,word in enumerate(text.split(' ')):\n    print_word+=word + ' '\n    i += 1\n    print_word =print_word + '\\n' if i%line_lenght == 0 else print_word\n\n  return print_word","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(clr.S+'Prompt'+clr.E)\nprint(format_text(dataset['Query'][0]))\nprint(clr.S+'\\nCompletition'+clr.E)\nprint(format_text(dataset['Response'][0]))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"formated_text = {'text':[]}\n\nbase_string = '''[INST]\nBelow is an instruction that describes a task.\nWrite a response that appropriately completes the request.\\n\\n\n{user_prompt}\\n\\n\n[/INST] {completition} </s>'''\n\nsystem_prompt = '''Below is an instruction that describes a task.\nWrite a response that appropriately completes the request.\\n\\n '''\n\ndataset = dataset.shuffle(seed=random.seed(CFG.seed))\n\nfor i,(Query, Response) in enumerate(zip(dataset['Query'],dataset['Response'])):\n    text = base_string.format(user_prompt = Query,\n                              completition = Response\n                              )\n    if i == 0:\n      text_example = clr.S+'[INST]\\n'+clr.E +\\\n            system_prompt + Query + '\\n\\n' +\\\n            clr.S+'[/INST] '+clr.E + Response + clr.S+' </s>'+clr.E\n\n    formated_text['text'].append(text)\n\n\nformate_dataset = Dataset.from_dict(formated_text)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"n_data = 10000 if CFG.debug else len(formate_dataset)\nn_data_train = int((1 - CFG.val_split) * n_data)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_dataset = Dataset.from_dict(formate_dataset[:n_data_train])\nvalid_dataset = Dataset.from_dict(formate_dataset[n_data_train:n_data])\n\nprint(clr.S+'Debug Mode = '+clr.E, CFG.debug)\nprint(clr.S+'\\nData Size'+clr.E)\nprint('Train = ',len(train_dataset), '| Valid = ', len(valid_dataset))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"### save HF Dataset to disk\ntrain_dataset.save_to_disk('./MedText-train')\nvalid_dataset.save_to_disk('./MedText-valid')\n### load HF Dataset from disk\ntrain_dataset = load_from_disk('./MedText-train')\nvalid_dataset = load_from_disk('./MedText-valid')\ntrain_dataset, valid_dataset","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"compute_dtype = getattr(torch, CFG.bnb_4bit_compute_dtype)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if compute_dtype == torch.float16 and CFG.use_4bit:\n    major, _ = torch.cuda.get_device_capability()\n    if major >= 8:\n        print(\"=\" * 80)\n        print(\"Your GPU supports bfloat16: accelerate training with bf16=True\")\n        print(\"=\" * 80)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"num_gpus = torch.cuda.device_count()\n# Get information about each GPU\nprint('General Report')\nfor i in range(num_gpus):\n    gpu_properties = torch.cuda.get_device_properties(i)\n    print(f\"\\nGPU {i}: {gpu_properties.name}\")\n    print(f\"  Total Memory: {gpu_properties.total_memory / (1024**3):.2f} GB\")\n    print(f\"  CUDA Version: {gpu_properties.major}.{gpu_properties.minor}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"bnb_config = BitsAndBytesConfig(\n    load_in_4bit=CFG.use_4bit,\n    bnb_4bit_quant_type=CFG.bnb_4bit_quant_type,\n    bnb_4bit_compute_dtype=compute_dtype,\n    bnb_4bit_use_double_quant=CFG.use_nested_quant,\n)\nprint('BitsAndBytesConfig\\n')\nbnb_config.to_dict()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#https://www.kaggle.com/code/hinepo/llm-instruction-finetuning-wandb#Model\ndef print_trainable_parameters(model):\n    \"\"\"\n    Prints the number of trainable parameters in the model.\n    \"\"\"\n    trainable_params = 0\n    all_param = 0\n    for _, param in model.named_parameters():\n        all_param += param.numel()\n        if param.requires_grad:\n            trainable_params += param.numel()\n\n    formatted_trainable_params = \"{:,}\".format(trainable_params).replace(\",\", \"_\")\n    formatted_all_param = \"{:,}\".format(all_param).replace(\",\", \"_\")\n    print(\n        f\"trainable params: {formatted_trainable_params} || all params: {formatted_all_param} || trainable: {round(100 * trainable_params / all_param, 2)} %\"\n    )","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%time\nmodel = AutoModelForCausalLM.from_pretrained(\n    CFG.model_name,\n    quantization_config=bnb_config,\n    device_map=CFG.device_map\n)\n\nmodel.config.use_cache = False\nmodel.config.pretraining_tp = 1\nprint_trainable_parameters(model)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(\n                            CFG.model_name,\n                            trust_remote_code=True\n                            )\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"right\" # Fix weird overflow issue with fp16 training","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.hf_device_map","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"layer_names = list(model.state_dict().keys())\n\nfor name in layer_names[4:10]:\n    print(name)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def find_all_linear_names(model):\n    cls = bnb.nn.Linear4bit\n    lora_module_names = set()\n    for name, module in model.named_modules():\n        if isinstance(module, cls):\n            names = name.split('.')\n            lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n    if 'lm_head' in lora_module_names:\n        lora_module_names.remove('lm_head')\n    return list(lora_module_names)\nlinear_layers = find_all_linear_names(model)\nprint(f\"Linear layers in the model: {linear_layers}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"peft_config = LoraConfig(\n    lora_alpha=CFG.lora_alpha,\n    lora_dropout=CFG.lora_dropout,\n    target_modules = linear_layers,\n    r=CFG.lora_r,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n)\npeft_config.to_dict()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.add_adapter(peft_config)\nprint_trainable_parameters(model)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"training_arguments = TrainingArguments(\n    output_dir=CFG.output_dir,\n    num_train_epochs=CFG.num_train_epochs,\n    per_device_train_batch_size=CFG.per_device_train_batch_size,\n    per_device_eval_batch_size=CFG.per_device_eval_batch_size,\n    gradient_accumulation_steps=CFG.gradient_accumulation_steps,\n    optim=CFG.optim,\n    learning_rate=CFG.learning_rate,\n    weight_decay=CFG.weight_decay,\n    fp16=CFG.fp16,\n    bf16=CFG.bf16,\n    max_grad_norm=CFG.max_grad_norm,\n    max_steps=CFG.max_steps,\n    warmup_ratio=CFG.warmup_ratio,\n    lr_scheduler_type=CFG.lr_scheduler_type,\n    save_strategy = 'steps',\n    save_steps=CFG.save_steps,\n    save_total_limit = 1,\n    gradient_checkpointing = True,\n    logging_strategy = \"steps\",\n    logging_steps=CFG.logging_steps,\n    evaluation_strategy = \"steps\",\n    eval_steps = CFG.logging_steps,\n    save_safetensors = True,\n    report_to=\"wandb\",\n    seed = CFG.seed,\n    data_seed = CFG.seed,\n    push_to_hub = False,\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class LLMSampleCB(WandbCallback):\n    def __init__(self, trainer, test_dataset, num_samples=10, max_new_tokens=256):\n        \"A Callback to log samples to a wandb.Table during training\"\n        super().__init__()\n\n        # Ensure the `log_model` is not passed as a string\n        self._log_model = None  # Use None or a proper model object if needed\n        \n        self.sample_dataset = test_dataset.select(range(num_samples))\n        self.model, self.tokenizer = trainer.model, trainer.tokenizer\n        \n        # Define the generation config\n        self.gen_config = transformers.GenerationConfig.from_pretrained(\n            trainer.model.name_or_path, max_new_tokens=max_new_tokens\n        )\n\n        # Initialize step tracking (to handle logging steps)\n        self.step = 1 if not os.path.exists(CFG.report_path) else pd.read_csv(CFG.report_path)['step'].max() + 1\n\n    def generate(self, instruction):\n        \"\"\"Generate a response for the given instruction.\"\"\"\n        tokenized_prompt = self.tokenizer(instruction, return_tensors='pt', padding=True).to('cuda')\n\n        with torch.inference_mode():\n            output = self.model.generate(\n                **tokenized_prompt,\n                generation_config=self.gen_config,\n                pad_token_id=self.tokenizer.eos_token_id\n            )\n\n        return self.tokenizer.decode(output[0][len(tokenized_prompt[0]):], skip_special_tokens=True)\n\n    def samples_table(self, examples):\n        \"\"\"Create a wandb.Table to store the generations.\"\"\"\n        if self.step in CFG.log_table_steps:\n            data = []\n            for example in examples:\n                prompt = example[\"text\"]\n                instruction = prompt.split('[/INST]')[0][6:]\n                response = prompt.split('[/INST]')[1][:-5]\n                generation = self.generate(instruction=instruction)\n                data.append([instruction, response, generation, *list(self.gen_config.to_dict().values())])\n\n            rerport_step_df = pd.DataFrame(data, columns=['instruction', 'response', 'generation'] + list(self.gen_config.to_dict().keys()))\n        else:\n            rerport_step_df = None\n\n        self.step += 1\n        return rerport_step_df\n\n    def on_evaluate(self, args, state, control, **kwargs):\n        \"\"\"Log the wandb.Table after calling trainer.evaluate.\"\"\"\n        super().on_evaluate(args, state, control, **kwargs)\n        \n        rerport_step_df = self.samples_table(self.sample_dataset)\n\n        # Update the table with predictions in the new step  \n        if rerport_step_df is not None:\n            if self.step - 1 == 1:\n                rerport_step_df['step'] = self.step - 1\n                rerport_step_df.to_csv(CFG.report_path, index=False)\n                new_report_df = rerport_step_df.copy()\n            else:\n                previous_report_df = pd.read_csv(CFG.report_path)\n                rerport_step_df['step'] = self.step - 1\n                new_report_df = pd.concat([previous_report_df, rerport_step_df], ignore_index=True)\n                new_report_df.to_csv(CFG.report_path, index=False)\n\n            table = wandb.Table(dataframe=new_report_df, allow_mixed_types=True)\n            wandb.log({\"sample_predictions\": table})","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Set supervised fine-tuning parameters\ntrainer = SFTTrainer(\n    model=model,\n    train_dataset=train_dataset,\n    eval_dataset = valid_dataset,\n    peft_config=peft_config,\n    dataset_text_field=\"text\",\n    max_seq_length=CFG.max_seq_length,\n    tokenizer=tokenizer,\n    args=training_arguments,\n    packing=CFG.packing,\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"wandb_callback = LLMSampleCB(trainer,\n                             valid_dataset,\n                             num_samples=5,\n                             max_new_tokens= 256)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"trainer.add_callback(wandb_callback)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_dataset","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(clr.S + 'All Train Steps = ' + clr.E, len(train_dataset)/CFG.per_device_train_batch_size)\nprint(clr.S + 'Max Train Steps = ' + clr.E, CFG.max_steps)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if not os.path.exists(CFG.output_dir):\n    os.makedirs(CFG.output_dir)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}